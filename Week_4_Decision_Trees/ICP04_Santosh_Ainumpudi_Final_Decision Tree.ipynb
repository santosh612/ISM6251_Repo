{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5344b43a",
   "metadata": {},
   "source": [
    "# ICP04 Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c0bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2d7c7",
   "metadata": {},
   "source": [
    "# 1.0 Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7950260",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('airbnb_train_X_price_gte_150.csv') \n",
    "y_train = pd.read_csv('airbnb_train_y_price_gte_150.csv') \n",
    "X_test = pd.read_csv('airbnb_test_X_price_gte_150.csv') \n",
    "y_test = pd.read_csv('airbnb_test_y_price_gte_150.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7581265",
   "metadata": {},
   "source": [
    "# 2.0 Model the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede630f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe88965",
   "metadata": {},
   "source": [
    "# 3.0 SVM Classification model using polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c02732",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly_model = SVC(kernel=\"poly\", degree=3, coef0=1, C=10)\n",
    "_ = svm_poly_model.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "svm_lin_model = SVC(kernel=\"linear\")\n",
    "_ = svm_lin_model.fit(X_train, np.ravel(y_train))\n",
    "model_preds = svm_poly_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm with polynomial kernel\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef91edd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.867854</td>\n",
       "      <td>0.855839</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.869323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  Accuracy  Precision    Recall        F1\n",
       "0  svm with polynomial kernel  0.867854   0.855839  0.883239  0.869323"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.sort_values(by=['Precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b54b90",
   "metadata": {},
   "source": [
    "# 4.0 DecisionTree Classifier:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3ea38",
   "metadata": {},
   "source": [
    "# 4.1 Random search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e2026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.8576881132722759\n",
      "... with parameters: {'min_samples_split': 32, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0031, 'max_leaf_nodes': 104, 'max_depth': 31, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.82488631 0.77844887 0.83581309 0.82488631 0.82488631 0.84243899\n",
      " 0.82778842 0.82488631 0.83581309 0.83556602 0.82488631 0.82488631\n",
      " 0.83280018 0.8280068  0.83820922 0.82704742 0.85672324 0.82488631\n",
      " 0.83194219 0.82793685 0.84494157 0.82627955 0.82488631 0.84380973\n",
      " 0.83453927 0.83212638 0.8280068  0.8317569  0.82488631 0.82488631\n",
      " 0.82488631 0.84381077 0.82627955 0.82488631 0.83541047        nan\n",
      " 0.82488631 0.82488631 0.82488631 0.83966532 0.82488631 0.84926223\n",
      " 0.83861123 0.84299949 0.82488631 0.82488631 0.83304336 0.77844887\n",
      " 0.83585906 0.83924658 0.8431623  0.84253064 0.8435269  0.83218559\n",
      " 0.82926049 0.82901807 0.84022191 0.82514459 0.82488631 0.82488631\n",
      " 0.83521469 0.82857656 0.8431623  0.85232707 0.82627955 0.84730066\n",
      " 0.83304336 0.83026191 0.83893887 0.82488631 0.82488631 0.84174215\n",
      " 0.84173701 0.83927765 0.82488631 0.82488631 0.82488631 0.82487201\n",
      " 0.82487201 0.83672945 0.84328803 0.8355079  0.82488631 0.82488631\n",
      " 0.82488631 0.82488631 0.8325115  0.82488631 0.82627955 0.8278815\n",
      " 0.8340782  0.82514459 0.8368684  0.84542834 0.82488631 0.83549985\n",
      " 0.82488631 0.82488631 0.83791315 0.77844887 0.83934553 0.82488631\n",
      " 0.82488631 0.82488631 0.82488631 0.82488631 0.82488631 0.83795471\n",
      " 0.82514459 0.83480762        nan 0.82488631 0.82488631 0.8352192\n",
      " 0.82578511 0.82488631 0.84649426 0.82488631 0.82488631 0.82488631\n",
      " 0.82514459 0.82488631 0.83603721 0.82488631 0.82778954 0.8280068\n",
      " 0.83556602 0.84036483 0.85734689 0.8280068  0.84062648 0.82488631\n",
      " 0.83478135 0.82841043 0.83581309 0.82488631 0.8505702  0.84189061\n",
      " 0.82857656 0.83581309 0.83433522 0.83585906 0.84276655 0.84523013\n",
      " 0.8389098  0.82488631 0.82768187 0.82857656 0.84035934 0.83304336\n",
      " 0.82488631 0.83433522 0.82488631 0.85303857 0.84863143 0.83271256\n",
      " 0.83231997 0.83536416 0.82488631 0.84116344 0.82488631 0.82828336\n",
      "        nan 0.77844887 0.83024761 0.83585906 0.82488631 0.83026191\n",
      " 0.82488631 0.83143504 0.83494371 0.84499722 0.83997506 0.83581309\n",
      " 0.84649426 0.82488631 0.8278815  0.82488631 0.83968159        nan\n",
      " 0.82488631 0.82488631 0.84287178 0.83364647 0.83026191 0.84173701\n",
      " 0.82488631 0.8280068  0.82634772 0.82869408 0.8278815  0.82488631\n",
      " 0.83690165 0.82487201 0.82627955 0.83242887 0.77844887 0.84708677\n",
      " 0.82488631 0.83564399 0.83581309 0.84276783 0.83026191 0.82857656\n",
      " 0.82488631 0.8411146  0.82566132 0.82488631 0.82488631 0.82514459\n",
      " 0.82829096 0.82488631 0.82586846 0.82488631 0.82488631 0.82488631\n",
      "        nan 0.82488631 0.84491204 0.82488631 0.84339679        nan\n",
      " 0.83886618 0.83847851 0.82488631 0.83639016 0.82488631 0.82904928\n",
      " 0.82514459 0.8388869  0.82566132 0.8280068  0.82488631 0.83302929\n",
      " 0.8539747  0.82488631 0.84708677 0.82488631 0.82488631 0.82857656\n",
      " 0.82488631 0.83609512 0.82768187 0.83687115 0.82488631 0.84679758\n",
      " 0.77844887 0.77844887 0.83581309 0.82514459 0.82575817 0.82488631\n",
      " 0.84769513 0.82855513 0.82488631 0.8280068  0.82488631 0.84649426\n",
      " 0.82488631 0.82488631 0.82488631 0.82487201 0.83304336 0.8360193\n",
      " 0.83133873 0.83987605 0.8429271  0.83567138 0.82768187 0.8280068\n",
      " 0.8280068  0.82488631 0.83470591 0.77844887 0.82488631 0.82488631\n",
      " 0.83304336 0.82488631 0.84543013 0.84602909 0.82858146 0.82586846\n",
      " 0.82926049 0.8278815  0.82488631 0.82693172 0.83304336 0.82488631\n",
      " 0.82488631 0.77844887 0.82488631 0.82488631 0.8403819  0.82768187\n",
      " 0.77844887 0.82841043 0.83773101 0.82488631 0.83498151 0.84269216\n",
      " 0.82488631 0.82488631 0.85513956 0.84544946 0.82959425 0.83581309\n",
      " 0.8312966  0.8317569  0.82487201 0.82768187 0.82488631 0.8450615\n",
      " 0.83581309 0.82488631 0.84523013 0.84053119 0.84062648 0.82488631\n",
      " 0.84276655 0.82431858 0.84194479 0.82586846 0.8437999  0.82884502\n",
      " 0.83299079 0.82488631 0.83321991 0.82514459 0.82627955 0.8280068\n",
      " 0.83825509 0.82488631 0.83585906 0.82509452 0.83917863 0.83118427\n",
      "        nan 0.82488631 0.82488631 0.84649426 0.82488631 0.83829554\n",
      " 0.82514459 0.82488631 0.82488631 0.82488631 0.8360193  0.83585906\n",
      " 0.82488631 0.84420114 0.85314645 0.84491204 0.8498319  0.82488631\n",
      " 0.82488631 0.82575817 0.83861803 0.82488631        nan 0.84085555\n",
      " 0.8278815  0.83856831 0.82488631 0.83212638 0.82488631 0.82488631\n",
      " 0.82488631 0.83585906 0.83957072 0.82488631 0.84679758 0.85768811\n",
      " 0.8280068  0.77844887 0.8312966  0.82488631 0.83218559 0.82488631\n",
      " 0.82488631 0.82488631 0.82514459        nan 0.82514459 0.83834604\n",
      " 0.85537012 0.82488631 0.8393849  0.82627955 0.82488631 0.82488631\n",
      " 0.77844887 0.8278815  0.83266762 0.83834604 0.83581309 0.82488631\n",
      " 0.82768187 0.82855513 0.82488631 0.84348217 0.82926049 0.84592853\n",
      " 0.83453927 0.82488631 0.83304336 0.83361517 0.8352192  0.83927765\n",
      " 0.82488631 0.82487201 0.82488631 0.82488631 0.82488631 0.83905714\n",
      " 0.84771674 0.83581309 0.82488631 0.84224584 0.82488631 0.82514459\n",
      " 0.82488631 0.82488631 0.84276655 0.82488631 0.82488631 0.82488631\n",
      " 0.82768187 0.8278815  0.83480762 0.8233934  0.83585906 0.77844887\n",
      " 0.82488631 0.83362618 0.82488631 0.82488631 0.83517315 0.82488631\n",
      " 0.84523013 0.82488631 0.82488631 0.77844887 0.82926049 0.83231997\n",
      " 0.82488631 0.82542903 0.82634772 0.84221652 0.77844887 0.82487201\n",
      " 0.82488631 0.83556602 0.82488631 0.82488631 0.82488631 0.8385913\n",
      " 0.82488631 0.82488631 0.82488631 0.83584946 0.82487201 0.8280068\n",
      " 0.84034455 0.82488631 0.83585906 0.83585906 0.84291253 0.82488631\n",
      " 0.83304336 0.8446265  0.83585906 0.85733585 0.83556602 0.84795638\n",
      " 0.82488631 0.82488631 0.8280068  0.82488631 0.82514459 0.84649426\n",
      " 0.83581309 0.8280068  0.77844887 0.82488631 0.82488631 0.82627955\n",
      " 0.84269394 0.82694982 0.84276655 0.82590832 0.82768187 0.82488631\n",
      " 0.82488631 0.83242887 0.82514459 0.82488631 0.82514459 0.85255081\n",
      " 0.82488631 0.83585906]\n",
      "  warnings.warn(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.82583601 0.77774902 0.84238417 0.82583601 0.82583601 0.87180944\n",
      " 0.86678805 0.82583601 0.84238417 0.8428255  0.82583601 0.82583601\n",
      " 0.8537422  0.83272552 0.85420408 0.90459519 0.88625992 0.82583601\n",
      " 0.86498612 0.85447315 0.85318962 0.83082791 0.82583601 0.86917853\n",
      " 0.85176411 0.84882477 0.83272552 0.83864725 0.82583601 0.82583601\n",
      " 0.82583601 0.85691878 0.83082791 0.82583601 0.84181549        nan\n",
      " 0.82583601 0.82583601 0.82583601 0.86545901 0.82583601 0.8678224\n",
      " 0.84625332 0.87640503 0.82583601 0.82583601 0.84049152 0.77774902\n",
      " 0.84341642 0.86253741 0.85897018 0.84850985 0.85286529 0.8379911\n",
      " 0.83397202 0.8488114  0.85412326 0.82851932 0.82583601 0.82583601\n",
      " 0.8589266  0.84022157 0.85897018 0.8705143  0.83082791 0.870187\n",
      " 0.84049152 0.83278796 0.84532065 0.82583601 0.82583601 0.849096\n",
      " 0.84544502 0.85631015 0.82583601 0.82583601 0.82583601 0.82814917\n",
      " 0.82814917 0.8567572  0.85342315 0.85914861 0.82583601 0.82583601\n",
      " 0.82583601 0.82583601 0.83800353 0.82583601 0.83082791 0.83301112\n",
      " 0.83899915 0.82851932 0.86950717 0.86092705 0.82583601 0.85829538\n",
      " 0.82583601 0.82583601 0.86920656 0.77774902 0.86694221 0.82583601\n",
      " 0.82583601 0.82583601 0.82583601 0.82583601 0.82583601 0.86693673\n",
      " 0.82851932 0.84214072        nan 0.82583601 0.82583601 0.84244045\n",
      " 0.82845794 0.82583601 0.85515207 0.82583601 0.82583601 0.82583601\n",
      " 0.82851932 0.82583601 0.88482903 0.82583601 0.85092662 0.83272552\n",
      " 0.8428255  0.86566422 0.88149104 0.83272552 0.84469235 0.82583601\n",
      " 0.85686556 0.83235552 0.84238417 0.82583601 0.87517374 0.84732778\n",
      " 0.84022157 0.84238417 0.83790805 0.84341642 0.84810619 0.85216545\n",
      " 0.84840174 0.82583601 0.83952466 0.84022157 0.85420662 0.84049152\n",
      " 0.82583601 0.83784463 0.82583601 0.87252517 0.87382901 0.83779112\n",
      " 0.8394209  0.85795121 0.82583601 0.86430707 0.82583601 0.86504793\n",
      "        nan 0.77774902 0.83506881 0.84341642 0.82583601 0.83278796\n",
      " 0.82583601 0.85627133 0.84140665 0.85231332 0.84827272 0.84238417\n",
      " 0.85498969 0.82583601 0.83301112 0.82583601 0.86454955        nan\n",
      " 0.82583601 0.82583601 0.85547508 0.83971985 0.83278796 0.84544502\n",
      " 0.82583601 0.83272552 0.84140421 0.83457867 0.83301112 0.82583601\n",
      " 0.86257137 0.82814917 0.83082791 0.85252173 0.77774902 0.85513746\n",
      " 0.82583601 0.85695423 0.84238417 0.85487675 0.83278796 0.84022157\n",
      " 0.82583601 0.86786928 0.82795927 0.82583601 0.82583601 0.82848866\n",
      " 0.84935738 0.82583601 0.83030851 0.82583601 0.82583601 0.82583601\n",
      "        nan 0.82583601 0.85280786 0.82583601 0.85324574        nan\n",
      " 0.85969102 0.86388028 0.82583601 0.86628988 0.82583601 0.83272057\n",
      " 0.82851932 0.86006269 0.82792791 0.83272552 0.82583601 0.83954919\n",
      " 0.87471552 0.82583601 0.85513746 0.82583601 0.82583601 0.84022157\n",
      " 0.82583601 0.84527682 0.83955617 0.85807156 0.82583601 0.85496571\n",
      " 0.77774902 0.77774902 0.84238417 0.82851932 0.83018155 0.82583601\n",
      " 0.88699307 0.83219021 0.82583601 0.83272552 0.82583601 0.85498969\n",
      " 0.82583601 0.82583601 0.82583601 0.82814917 0.84049152 0.84408099\n",
      " 0.83634263 0.84373893 0.85378376 0.85966303 0.83955617 0.83272552\n",
      " 0.83272552 0.82583601 0.84072084 0.77774902 0.82583601 0.82583601\n",
      " 0.84049152 0.82583601 0.86407617 0.86727615 0.90879737 0.83087084\n",
      " 0.83397202 0.83301112 0.82583601 0.85017143 0.84049152 0.82583601\n",
      " 0.82583601 0.77774902 0.82583601 0.82583601 0.87273471 0.83955617\n",
      " 0.77774902 0.83235552 0.84818137 0.82583601 0.85834674 0.84997759\n",
      " 0.82583601 0.82583601 0.8675049  0.85774412 0.83445936 0.84238417\n",
      " 0.83504222 0.83864725 0.82814917 0.83955617 0.82583601 0.85995571\n",
      " 0.84238417 0.82583601 0.85216545 0.87947656 0.84469235 0.82583601\n",
      " 0.84810619 0.82574677 0.86612789 0.83080794 0.85950832 0.90253711\n",
      " 0.8940645  0.82583601 0.89839814 0.82851932 0.83082791 0.83272552\n",
      " 0.84786933 0.82583601 0.84341642 0.84688781 0.8633291  0.8367705\n",
      "        nan 0.82583601 0.82583601 0.85515207 0.82583601 0.86162752\n",
      " 0.82851932 0.82583601 0.82583601 0.82583601 0.84408099 0.84341642\n",
      " 0.82583601 0.85846753 0.87294401 0.85280786 0.86895721 0.82583601\n",
      " 0.82583601 0.83018155 0.84850756 0.82583601        nan 0.86434512\n",
      " 0.83301112 0.8437397  0.82583601 0.84917181 0.82583601 0.82583601\n",
      " 0.82583601 0.84341642 0.86867676 0.82583601 0.85496571 0.87880342\n",
      " 0.83272552 0.77774902 0.83504222 0.82583601 0.8379911  0.82583601\n",
      " 0.82583601 0.82583601 0.82851932        nan 0.82851932 0.84634707\n",
      " 0.86875616 0.82583601 0.86631975 0.83082791 0.82583601 0.82583601\n",
      " 0.77774902 0.83301112 0.83921122 0.84634707 0.84238417 0.82583601\n",
      " 0.83940381 0.83219021 0.82583601 0.86433602 0.83397202 0.85570267\n",
      " 0.85176411 0.82583601 0.84049152 0.84394939 0.84244045 0.85612207\n",
      " 0.82583601 0.82814917 0.82583601 0.82583601 0.82583601 0.86046327\n",
      " 0.87175973 0.84238417 0.82583601 0.85416438 0.82583601 0.82851932\n",
      " 0.82583601 0.82583601 0.84810619 0.82583601 0.82583601 0.82583601\n",
      " 0.83955617 0.83301112 0.84214072 0.84531173 0.84341642 0.77774902\n",
      " 0.82583601 0.83879513 0.82583601 0.82583601 0.85745189 0.82583601\n",
      " 0.85216545 0.82583601 0.82583601 0.77774902 0.83397202 0.8394209\n",
      " 0.82583601 0.86016923 0.84140421 0.84744981 0.77774902 0.82814917\n",
      " 0.82583601 0.8428255  0.82583601 0.82583601 0.82583601 0.84573211\n",
      " 0.82583601 0.82583601 0.82583601 0.87128744 0.82814917 0.83272552\n",
      " 0.87359698 0.82583601 0.84341642 0.84341642 0.85455366 0.82583601\n",
      " 0.84049152 0.85328039 0.84341642 0.87891781 0.8428255  0.86344941\n",
      " 0.82583601 0.82583601 0.83272552 0.82583601 0.82848866 0.85515207\n",
      " 0.84238417 0.83272552 0.77774902 0.82583601 0.82583601 0.83082791\n",
      " 0.87884234 0.87353497 0.84810619 0.86924223 0.83955617 0.82583601\n",
      " 0.82583601 0.85252173 0.82851932 0.82583601 0.82848866 0.869247\n",
      " 0.82583601 0.84341642]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,55),  \n",
    "    'min_samples_leaf': np.arange(1,55),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2980cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8444236 Precision=0.8489484 Recall=0.8361582 F1=0.8425047\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb6079",
   "metadata": {},
   "source": [
    "# 4.2 Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac2aa191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3125 candidates, totalling 15625 fits\n",
      "The best precision score is 0.8589345555212112\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 30, 'max_leaf_nodes': 100, 'min_impurity_decrease': 0.0031999999999999993, 'min_samples_leaf': 7, 'min_samples_split': 34}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,35),  \n",
    "    'min_samples_leaf': np.arange(5,10),\n",
    "    'min_impurity_decrease': np.arange(0.0028, 0.0033, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(100,105), \n",
    "    'max_depth': np.arange(30,35), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "984422fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8472352 Precision=0.8552124 Recall=0.8342750 F1=0.8446139\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d474a",
   "metadata": {},
   "source": [
    "When we compare all of the precision values, we see that the SVM model with polynomial is the best fit model, with a precision value of 0.855839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0728b7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad9309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
