{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72583252",
   "metadata": {},
   "source": [
    "# Santosh Ainumpudi -U68091846"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88a212",
   "metadata": {},
   "source": [
    "Parkinson's disease is a serious neurological condition that requires accurate and timely diagnosis. Therefore, the primary goal of any model developed for Parkinson's disease diagnosis should be to maximize accuracy.\n",
    "The dataset is a balanced dataset, meaning that there are an equal number of samples for both positive and negative classes. In such cases, accuracy is a reliable and appropriate metric to evaluate the performance of the model.\n",
    "Accuracy provides a simple and easy-to-understand measure of the model's overall performance. It tells us the proportion of correctly classified instances to the total number of instances, which is a useful measure for assessing the model's effectiveness.We need to accurately predict the presence of Parkinson disease as 1 or 0 (i.e as Yes or No)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7268087",
   "metadata": {},
   "source": [
    "The choice of the best scoring metric for the Parkinson's disease dataset is that as it depends on the distribution of the target variable, and the relative importance of true positives and false positives or true negatives and false negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6e232",
   "metadata": {},
   "source": [
    "Since goal is to accurately predict the presence or absence of Parkinson's disease, I am taking accuracy as the scoring metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8c0c2",
   "metadata": {},
   "source": [
    "# Fitting the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a2742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52492df2",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9adee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('parkinsons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6906c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>status</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phon_R01_S01_1</td>\n",
       "      <td>119.992</td>\n",
       "      <td>157.302</td>\n",
       "      <td>74.997</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00370</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>0.01109</td>\n",
       "      <td>0.04374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.02211</td>\n",
       "      <td>21.033</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414783</td>\n",
       "      <td>0.815285</td>\n",
       "      <td>-4.813031</td>\n",
       "      <td>0.266482</td>\n",
       "      <td>2.301442</td>\n",
       "      <td>0.284654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phon_R01_S01_2</td>\n",
       "      <td>122.400</td>\n",
       "      <td>148.650</td>\n",
       "      <td>113.819</td>\n",
       "      <td>0.00968</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.00696</td>\n",
       "      <td>0.01394</td>\n",
       "      <td>0.06134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09403</td>\n",
       "      <td>0.01929</td>\n",
       "      <td>19.085</td>\n",
       "      <td>1</td>\n",
       "      <td>0.458359</td>\n",
       "      <td>0.819521</td>\n",
       "      <td>-4.075192</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>2.486855</td>\n",
       "      <td>0.368674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phon_R01_S01_3</td>\n",
       "      <td>116.682</td>\n",
       "      <td>131.111</td>\n",
       "      <td>111.555</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>0.00781</td>\n",
       "      <td>0.01633</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08270</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>20.651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.429895</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-4.443179</td>\n",
       "      <td>0.311173</td>\n",
       "      <td>2.342259</td>\n",
       "      <td>0.332634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phon_R01_S01_4</td>\n",
       "      <td>116.676</td>\n",
       "      <td>137.871</td>\n",
       "      <td>111.366</td>\n",
       "      <td>0.00997</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.00698</td>\n",
       "      <td>0.01505</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08771</td>\n",
       "      <td>0.01353</td>\n",
       "      <td>20.644</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434969</td>\n",
       "      <td>0.819235</td>\n",
       "      <td>-4.117501</td>\n",
       "      <td>0.334147</td>\n",
       "      <td>2.405554</td>\n",
       "      <td>0.368975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phon_R01_S01_5</td>\n",
       "      <td>116.014</td>\n",
       "      <td>141.781</td>\n",
       "      <td>110.655</td>\n",
       "      <td>0.01284</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00655</td>\n",
       "      <td>0.00908</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>0.06425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.01767</td>\n",
       "      <td>19.649</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>-3.747787</td>\n",
       "      <td>0.234513</td>\n",
       "      <td>2.332180</td>\n",
       "      <td>0.410335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "0  phon_R01_S01_1      119.992       157.302        74.997         0.00784   \n",
       "1  phon_R01_S01_2      122.400       148.650       113.819         0.00968   \n",
       "2  phon_R01_S01_3      116.682       131.111       111.555         0.01050   \n",
       "3  phon_R01_S01_4      116.676       137.871       111.366         0.00997   \n",
       "4  phon_R01_S01_5      116.014       141.781       110.655         0.01284   \n",
       "\n",
       "   MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  ...  \\\n",
       "0           0.00007   0.00370   0.00554     0.01109       0.04374  ...   \n",
       "1           0.00008   0.00465   0.00696     0.01394       0.06134  ...   \n",
       "2           0.00009   0.00544   0.00781     0.01633       0.05233  ...   \n",
       "3           0.00009   0.00502   0.00698     0.01505       0.05492  ...   \n",
       "4           0.00011   0.00655   0.00908     0.01966       0.06425  ...   \n",
       "\n",
       "   Shimmer:DDA      NHR     HNR  status      RPDE       DFA   spread1  \\\n",
       "0      0.06545  0.02211  21.033       1  0.414783  0.815285 -4.813031   \n",
       "1      0.09403  0.01929  19.085       1  0.458359  0.819521 -4.075192   \n",
       "2      0.08270  0.01309  20.651       1  0.429895  0.825288 -4.443179   \n",
       "3      0.08771  0.01353  20.644       1  0.434969  0.819235 -4.117501   \n",
       "4      0.10470  0.01767  19.649       1  0.417356  0.823484 -3.747787   \n",
       "\n",
       "    spread2        D2       PPE  \n",
       "0  0.266482  2.301442  0.284654  \n",
       "1  0.335590  2.486855  0.368674  \n",
       "2  0.311173  2.342259  0.332634  \n",
       "3  0.334147  2.405554  0.368975  \n",
       "4  0.234513  2.332180  0.410335  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed44e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195 entries, 0 to 194\n",
      "Data columns (total 24 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   name              195 non-null    object \n",
      " 1   MDVP:Fo(Hz)       195 non-null    float64\n",
      " 2   MDVP:Fhi(Hz)      195 non-null    float64\n",
      " 3   MDVP:Flo(Hz)      195 non-null    float64\n",
      " 4   MDVP:Jitter(%)    195 non-null    float64\n",
      " 5   MDVP:Jitter(Abs)  195 non-null    float64\n",
      " 6   MDVP:RAP          195 non-null    float64\n",
      " 7   MDVP:PPQ          195 non-null    float64\n",
      " 8   Jitter:DDP        195 non-null    float64\n",
      " 9   MDVP:Shimmer      195 non-null    float64\n",
      " 10  MDVP:Shimmer(dB)  195 non-null    float64\n",
      " 11  Shimmer:APQ3      195 non-null    float64\n",
      " 12  Shimmer:APQ5      195 non-null    float64\n",
      " 13  MDVP:APQ          195 non-null    float64\n",
      " 14  Shimmer:DDA       195 non-null    float64\n",
      " 15  NHR               195 non-null    float64\n",
      " 16  HNR               195 non-null    float64\n",
      " 17  status            195 non-null    int64  \n",
      " 18  RPDE              195 non-null    float64\n",
      " 19  DFA               195 non-null    float64\n",
      " 20  spread1           195 non-null    float64\n",
      " 21  spread2           195 non-null    float64\n",
      " 22  D2                195 non-null    float64\n",
      " 23  PPE               195 non-null    float64\n",
      "dtypes: float64(22), int64(1), object(1)\n",
      "memory usage: 36.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d7e295",
   "metadata": {},
   "source": [
    "The target variable is the \"status\" column, which is an integer indicating the presence or absence of Parkinson's disease in the patient. The input variables are all the other columns except for the \"name\" column, which just contains the name of the patient and is not relevant for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63eb7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa18b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195 entries, 0 to 194\n",
      "Data columns (total 23 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   MDVP:Fo(Hz)       195 non-null    float64\n",
      " 1   MDVP:Fhi(Hz)      195 non-null    float64\n",
      " 2   MDVP:Flo(Hz)      195 non-null    float64\n",
      " 3   MDVP:Jitter(%)    195 non-null    float64\n",
      " 4   MDVP:Jitter(Abs)  195 non-null    float64\n",
      " 5   MDVP:RAP          195 non-null    float64\n",
      " 6   MDVP:PPQ          195 non-null    float64\n",
      " 7   Jitter:DDP        195 non-null    float64\n",
      " 8   MDVP:Shimmer      195 non-null    float64\n",
      " 9   MDVP:Shimmer(dB)  195 non-null    float64\n",
      " 10  Shimmer:APQ3      195 non-null    float64\n",
      " 11  Shimmer:APQ5      195 non-null    float64\n",
      " 12  MDVP:APQ          195 non-null    float64\n",
      " 13  Shimmer:DDA       195 non-null    float64\n",
      " 14  NHR               195 non-null    float64\n",
      " 15  HNR               195 non-null    float64\n",
      " 16  status            195 non-null    int64  \n",
      " 17  RPDE              195 non-null    float64\n",
      " 18  DFA               195 non-null    float64\n",
      " 19  spread1           195 non-null    float64\n",
      " 20  spread2           195 non-null    float64\n",
      " 21  D2                195 non-null    float64\n",
      " 22  PPE               195 non-null    float64\n",
      "dtypes: float64(22), int64(1)\n",
      "memory usage: 35.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87769aa",
   "metadata": {},
   "source": [
    "Most of the columns are of type float64 i.e 22 of them , with one column (status) of type int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb8519a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>MDVP:Shimmer(dB)</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>status</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>154.228641</td>\n",
       "      <td>197.104918</td>\n",
       "      <td>116.324631</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>0.009920</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>0.282251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046993</td>\n",
       "      <td>0.024847</td>\n",
       "      <td>21.885974</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.498536</td>\n",
       "      <td>0.718099</td>\n",
       "      <td>-5.684397</td>\n",
       "      <td>0.226510</td>\n",
       "      <td>2.381826</td>\n",
       "      <td>0.206552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>41.390065</td>\n",
       "      <td>91.491548</td>\n",
       "      <td>43.521413</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.008903</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>0.194877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030459</td>\n",
       "      <td>0.040418</td>\n",
       "      <td>4.425764</td>\n",
       "      <td>0.431878</td>\n",
       "      <td>0.103942</td>\n",
       "      <td>0.055336</td>\n",
       "      <td>1.090208</td>\n",
       "      <td>0.083406</td>\n",
       "      <td>0.382799</td>\n",
       "      <td>0.090119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>88.333000</td>\n",
       "      <td>102.145000</td>\n",
       "      <td>65.476000</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>8.441000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256570</td>\n",
       "      <td>0.574282</td>\n",
       "      <td>-7.964984</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>1.423287</td>\n",
       "      <td>0.044539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>117.572000</td>\n",
       "      <td>134.862500</td>\n",
       "      <td>84.291000</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>0.005925</td>\n",
       "      <td>19.198000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421306</td>\n",
       "      <td>0.674758</td>\n",
       "      <td>-6.450096</td>\n",
       "      <td>0.174351</td>\n",
       "      <td>2.099125</td>\n",
       "      <td>0.137451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>148.790000</td>\n",
       "      <td>175.829000</td>\n",
       "      <td>104.315000</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.022970</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038360</td>\n",
       "      <td>0.011660</td>\n",
       "      <td>22.085000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.495954</td>\n",
       "      <td>0.722254</td>\n",
       "      <td>-5.720868</td>\n",
       "      <td>0.218885</td>\n",
       "      <td>2.361532</td>\n",
       "      <td>0.194052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>182.769000</td>\n",
       "      <td>224.205500</td>\n",
       "      <td>140.018500</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.011505</td>\n",
       "      <td>0.037885</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060795</td>\n",
       "      <td>0.025640</td>\n",
       "      <td>25.075500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587562</td>\n",
       "      <td>0.761881</td>\n",
       "      <td>-5.046192</td>\n",
       "      <td>0.279234</td>\n",
       "      <td>2.636456</td>\n",
       "      <td>0.252980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>260.105000</td>\n",
       "      <td>592.030000</td>\n",
       "      <td>239.170000</td>\n",
       "      <td>0.033160</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>0.019580</td>\n",
       "      <td>0.064330</td>\n",
       "      <td>0.119080</td>\n",
       "      <td>1.302000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169420</td>\n",
       "      <td>0.314820</td>\n",
       "      <td>33.047000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685151</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-2.434031</td>\n",
       "      <td>0.450493</td>\n",
       "      <td>3.671155</td>\n",
       "      <td>0.527367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "count   195.000000    195.000000    195.000000      195.000000   \n",
       "mean    154.228641    197.104918    116.324631        0.006220   \n",
       "std      41.390065     91.491548     43.521413        0.004848   \n",
       "min      88.333000    102.145000     65.476000        0.001680   \n",
       "25%     117.572000    134.862500     84.291000        0.003460   \n",
       "50%     148.790000    175.829000    104.315000        0.004940   \n",
       "75%     182.769000    224.205500    140.018500        0.007365   \n",
       "max     260.105000    592.030000    239.170000        0.033160   \n",
       "\n",
       "       MDVP:Jitter(Abs)    MDVP:RAP    MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  \\\n",
       "count        195.000000  195.000000  195.000000  195.000000    195.000000   \n",
       "mean           0.000044    0.003306    0.003446    0.009920      0.029709   \n",
       "std            0.000035    0.002968    0.002759    0.008903      0.018857   \n",
       "min            0.000007    0.000680    0.000920    0.002040      0.009540   \n",
       "25%            0.000020    0.001660    0.001860    0.004985      0.016505   \n",
       "50%            0.000030    0.002500    0.002690    0.007490      0.022970   \n",
       "75%            0.000060    0.003835    0.003955    0.011505      0.037885   \n",
       "max            0.000260    0.021440    0.019580    0.064330      0.119080   \n",
       "\n",
       "       MDVP:Shimmer(dB)  ...  Shimmer:DDA         NHR         HNR      status  \\\n",
       "count        195.000000  ...   195.000000  195.000000  195.000000  195.000000   \n",
       "mean           0.282251  ...     0.046993    0.024847   21.885974    0.753846   \n",
       "std            0.194877  ...     0.030459    0.040418    4.425764    0.431878   \n",
       "min            0.085000  ...     0.013640    0.000650    8.441000    0.000000   \n",
       "25%            0.148500  ...     0.024735    0.005925   19.198000    1.000000   \n",
       "50%            0.221000  ...     0.038360    0.011660   22.085000    1.000000   \n",
       "75%            0.350000  ...     0.060795    0.025640   25.075500    1.000000   \n",
       "max            1.302000  ...     0.169420    0.314820   33.047000    1.000000   \n",
       "\n",
       "             RPDE         DFA     spread1     spread2          D2         PPE  \n",
       "count  195.000000  195.000000  195.000000  195.000000  195.000000  195.000000  \n",
       "mean     0.498536    0.718099   -5.684397    0.226510    2.381826    0.206552  \n",
       "std      0.103942    0.055336    1.090208    0.083406    0.382799    0.090119  \n",
       "min      0.256570    0.574282   -7.964984    0.006274    1.423287    0.044539  \n",
       "25%      0.421306    0.674758   -6.450096    0.174351    2.099125    0.137451  \n",
       "50%      0.495954    0.722254   -5.720868    0.218885    2.361532    0.194052  \n",
       "75%      0.587562    0.761881   -5.046192    0.279234    2.636456    0.252980  \n",
       "max      0.685151    0.825288   -2.434031    0.450493    3.671155    0.527367  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3f5d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)',\n",
       "       'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP',\n",
       "       'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',\n",
       "       'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'status', 'RPDE', 'DFA',\n",
       "       'spread1', 'spread2', 'D2', 'PPE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5665f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b74a6",
   "metadata": {},
   "source": [
    "# Find the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b6ba9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDVP:Fo(Hz)         0\n",
       "MDVP:Fhi(Hz)        0\n",
       "MDVP:Flo(Hz)        0\n",
       "MDVP:Jitter(%)      0\n",
       "MDVP:Jitter(Abs)    0\n",
       "MDVP:RAP            0\n",
       "MDVP:PPQ            0\n",
       "Jitter:DDP          0\n",
       "MDVP:Shimmer        0\n",
       "MDVP:Shimmer(dB)    0\n",
       "Shimmer:APQ3        0\n",
       "Shimmer:APQ5        0\n",
       "MDVP:APQ            0\n",
       "Shimmer:DDA         0\n",
       "NHR                 0\n",
       "HNR                 0\n",
       "status              0\n",
       "RPDE                0\n",
       "DFA                 0\n",
       "spread1             0\n",
       "spread2             0\n",
       "D2                  0\n",
       "PPE                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a2afb",
   "metadata": {},
   "source": [
    "The df.isnull().sum() method returns the number of missing values in each column of the DataFrame. In this case, all columns have zero missing values, which indicates that the data is complete and ready for processing. Therefore, we can proceed with our analysis without having to impute or drop any missing values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0f4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['status'], axis=1)\n",
    "y = df['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0a842",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d4a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4182c8d",
   "metadata": {},
   "source": [
    "# Modelling the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928ab0b",
   "metadata": {},
   "source": [
    "# Logistic Regession using Random Search and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e43411",
   "metadata": {},
   "source": [
    "# Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "433b774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 700 candidates, totalling 2100 fits\n",
      "The accuracy score is 0.8528180354267311 with {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 381, 'C': 10} parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "783 fits failed out of a total of 2100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "279 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "213 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "291 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.75732689 0.80161031 0.74991948 0.75732689        nan        nan\n",
      " 0.74991948        nan        nan 0.74991948 0.74991948 0.74991948\n",
      " 0.80161031 0.75732689 0.75732689        nan        nan 0.75732689\n",
      "        nan 0.74991948 0.74991948 0.75732689 0.74991948 0.80161031\n",
      "        nan        nan        nan        nan 0.7647343         nan\n",
      "        nan 0.75732689 0.85281804        nan 0.75732689 0.75732689\n",
      "        nan        nan 0.78695652 0.84557166 0.75732689 0.75732689\n",
      "        nan 0.75732689 0.75732689        nan 0.74991948        nan\n",
      " 0.74991948 0.85281804        nan        nan        nan        nan\n",
      " 0.7647343  0.75732689        nan 0.75732689 0.74991948 0.85249597\n",
      " 0.74991948 0.75732689 0.84557166        nan        nan 0.75732689\n",
      " 0.75732689        nan 0.74991948        nan        nan 0.74991948\n",
      " 0.75732689        nan 0.75732689 0.75732689 0.78695652 0.74991948\n",
      " 0.75732689 0.7647343  0.7647343         nan        nan 0.75732689\n",
      " 0.75732689 0.85281804 0.75732689        nan 0.75732689        nan\n",
      " 0.75732689 0.7647343         nan 0.75732689        nan 0.7647343\n",
      " 0.75732689 0.74991948 0.74991948 0.74991948 0.75732689 0.7647343\n",
      " 0.7647343         nan        nan 0.75732689 0.80161031        nan\n",
      " 0.7647343  0.75732689 0.84557166 0.75732689 0.75732689 0.75732689\n",
      " 0.75732689        nan        nan 0.74991948        nan 0.75732689\n",
      " 0.74991948 0.74991948 0.75732689 0.75732689 0.74991948 0.75732689\n",
      " 0.74991948        nan 0.75732689 0.75732689        nan        nan\n",
      " 0.75732689 0.75732689 0.74991948 0.74991948 0.75732689        nan\n",
      " 0.75732689        nan 0.80161031 0.852657   0.75732689 0.852657\n",
      " 0.75732689        nan        nan 0.74991948 0.74991948 0.7647343\n",
      " 0.75732689        nan        nan 0.7647343  0.75732689 0.75732689\n",
      " 0.75732689 0.75732689        nan 0.85281804        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.74991948\n",
      "        nan 0.7647343  0.80161031 0.75732689 0.74991948        nan\n",
      " 0.78695652 0.75732689        nan 0.75732689 0.75732689 0.75732689\n",
      " 0.75732689        nan        nan 0.75732689 0.74991948        nan\n",
      "        nan 0.7647343  0.75732689 0.75732689        nan 0.75732689\n",
      " 0.75732689        nan        nan 0.75732689        nan        nan\n",
      "        nan        nan 0.75732689        nan 0.75732689        nan\n",
      "        nan        nan        nan 0.75732689        nan 0.75732689\n",
      "        nan 0.74991948 0.75732689        nan 0.78695652 0.75732689\n",
      "        nan 0.74991948        nan        nan 0.75732689        nan\n",
      " 0.74991948 0.75732689 0.78695652 0.78695652 0.7647343         nan\n",
      " 0.75732689 0.7647343  0.74991948        nan        nan 0.75732689\n",
      "        nan 0.75732689 0.75732689        nan 0.75732689        nan\n",
      " 0.7647343         nan        nan        nan        nan        nan\n",
      "        nan 0.80161031 0.7647343  0.75732689 0.75732689        nan\n",
      " 0.75732689        nan 0.74991948 0.75732689 0.852657   0.74991948\n",
      "        nan 0.78695652        nan        nan 0.75732689 0.74991948\n",
      " 0.74991948 0.85249597 0.74991948 0.74991948 0.75732689        nan\n",
      " 0.74991948 0.78695652 0.75732689        nan 0.74991948        nan\n",
      " 0.75732689 0.7647343  0.74991948 0.74991948        nan 0.75732689\n",
      " 0.75732689        nan 0.852657          nan 0.75732689 0.75732689\n",
      "        nan        nan 0.75732689 0.85281804 0.85249597        nan\n",
      " 0.80161031 0.75732689        nan 0.74991948 0.75732689 0.75732689\n",
      "        nan 0.852657   0.84557166 0.75732689 0.75732689        nan\n",
      " 0.75732689 0.75732689 0.74991948 0.75732689 0.75732689        nan\n",
      "        nan 0.852657   0.75732689 0.75732689        nan 0.75732689\n",
      " 0.75732689 0.75732689        nan        nan        nan 0.75732689\n",
      "        nan        nan 0.7647343  0.75732689 0.75732689 0.74991948\n",
      " 0.85249597 0.74991948        nan        nan 0.75732689 0.85249597\n",
      "        nan 0.85281804 0.74991948        nan 0.75732689 0.74991948\n",
      "        nan 0.75732689        nan 0.74991948        nan 0.80161031\n",
      " 0.75732689        nan 0.75732689        nan 0.74991948        nan\n",
      " 0.75732689        nan 0.75732689 0.74991948        nan        nan\n",
      " 0.75732689 0.75732689 0.75732689 0.75732689 0.74991948        nan\n",
      " 0.75732689        nan        nan 0.75732689 0.75732689 0.85281804\n",
      " 0.75732689        nan 0.75732689 0.75732689 0.75732689        nan\n",
      "        nan 0.75732689 0.78695652 0.75732689        nan        nan\n",
      "        nan 0.75732689 0.75732689 0.75732689        nan        nan\n",
      "        nan 0.75732689 0.74991948 0.7647343  0.80161031        nan\n",
      "        nan 0.75732689 0.74991948 0.75732689 0.75732689 0.75732689\n",
      "        nan 0.7647343  0.75732689 0.74991948        nan        nan\n",
      " 0.75732689 0.74991948 0.78695652 0.75732689 0.85249597 0.74991948\n",
      " 0.74991948        nan 0.75732689 0.85249597 0.75732689 0.7647343\n",
      " 0.75732689        nan 0.74991948 0.85249597 0.84557166        nan\n",
      " 0.75732689 0.80161031        nan        nan 0.74991948 0.75732689\n",
      "        nan 0.7647343  0.75732689 0.75732689 0.75732689 0.852657\n",
      " 0.75732689 0.75732689 0.85249597        nan 0.7647343  0.74991948\n",
      "        nan 0.75732689 0.7647343  0.74991948 0.74991948 0.74991948\n",
      " 0.75732689 0.75732689        nan 0.74991948 0.75732689 0.75732689\n",
      "        nan        nan 0.80161031        nan        nan        nan\n",
      "        nan 0.75732689 0.85249597 0.75732689        nan        nan\n",
      "        nan 0.7647343         nan 0.75732689        nan 0.75732689\n",
      " 0.74991948 0.75732689        nan        nan 0.74991948 0.75732689\n",
      " 0.75732689 0.74991948 0.852657          nan        nan        nan\n",
      "        nan 0.7647343  0.78695652 0.74991948 0.75732689 0.84557166\n",
      " 0.74991948 0.78695652        nan 0.75732689 0.78695652        nan\n",
      "        nan        nan 0.74991948 0.75732689 0.80161031        nan\n",
      "        nan 0.75732689        nan 0.75732689        nan        nan\n",
      " 0.75732689        nan        nan 0.852657   0.75732689 0.75732689\n",
      "        nan 0.75732689        nan        nan 0.75732689 0.74991948\n",
      " 0.75732689 0.84557166 0.80161031 0.74991948        nan        nan\n",
      " 0.74991948        nan        nan 0.7647343  0.75732689        nan\n",
      "        nan        nan 0.74991948 0.75732689        nan 0.85281804\n",
      "        nan 0.75732689        nan 0.74991948 0.75732689        nan\n",
      "        nan 0.75732689 0.7647343  0.74991948 0.852657          nan\n",
      " 0.74991948 0.75732689 0.75732689 0.75732689 0.75732689 0.75732689\n",
      "        nan        nan 0.75732689 0.85249597 0.74991948        nan\n",
      "        nan 0.75732689 0.80161031 0.7647343  0.75732689 0.75732689\n",
      "        nan        nan        nan 0.80161031        nan        nan\n",
      " 0.75732689 0.74991948        nan 0.7647343  0.85249597 0.75732689\n",
      "        nan        nan 0.75732689 0.75732689        nan 0.852657\n",
      "        nan        nan        nan        nan 0.84557166        nan\n",
      " 0.852657          nan 0.85281804 0.75732689        nan 0.7647343\n",
      "        nan        nan 0.7647343  0.80161031 0.75732689 0.74991948\n",
      " 0.84557166 0.75732689 0.85249597 0.85281804 0.85249597 0.74991948\n",
      "        nan 0.75732689        nan 0.75732689 0.7647343  0.75732689\n",
      " 0.75732689        nan 0.85281804 0.75732689        nan        nan\n",
      "        nan 0.75732689 0.7647343  0.75732689 0.75732689 0.75732689\n",
      "        nan 0.75732689 0.85281804        nan        nan 0.75732689\n",
      "        nan 0.7647343         nan        nan 0.7647343         nan\n",
      " 0.7647343  0.7647343         nan        nan 0.74991948        nan\n",
      " 0.75732689 0.74991948        nan        nan 0.85281804        nan\n",
      " 0.75732689 0.74991948        nan 0.75732689 0.85249597 0.80161031\n",
      " 0.75732689 0.74991948        nan        nan 0.74991948 0.75732689\n",
      "        nan 0.75732689        nan        nan        nan        nan\n",
      " 0.74991948 0.75732689 0.74991948 0.75732689 0.7647343         nan\n",
      " 0.75732689 0.75732689        nan        nan 0.75732689 0.74991948\n",
      " 0.852657   0.74991948 0.75732689 0.74991948        nan 0.852657\n",
      " 0.75732689 0.75732689        nan        nan 0.74991948 0.75732689\n",
      " 0.75732689 0.80161031        nan 0.75732689]\n",
      "  warnings.warn(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.75734636 0.84542125 0.76100936 0.75734636        nan        nan\n",
      " 0.76100936        nan        nan 0.76100936 0.76100936 0.76100936\n",
      " 0.84542125 0.76100936 0.75734636        nan        nan 0.75734636\n",
      "        nan 0.76100936 0.76100936 0.75734636 0.76100936 0.84542125\n",
      "        nan        nan        nan        nan 0.76100936        nan\n",
      "        nan 0.75734636 0.86027676        nan 0.75734636 0.75734636\n",
      "        nan        nan 0.81595442 0.86760277 0.75734636 0.75734636\n",
      "        nan 0.75734636 0.75734636        nan 0.76100936        nan\n",
      " 0.76100936 0.86027676        nan        nan        nan        nan\n",
      " 0.76100936 0.75734636        nan 0.75734636 0.76100936 0.87118437\n",
      " 0.76100936 0.75734636 0.86760277        nan        nan 0.75734636\n",
      " 0.75734636        nan 0.76100936        nan        nan 0.76100936\n",
      " 0.75734636        nan 0.75734636 0.76100936 0.81595442 0.76100936\n",
      " 0.75734636 0.76100936 0.76100936        nan        nan 0.76100936\n",
      " 0.75734636 0.86027676 0.75734636        nan 0.75734636        nan\n",
      " 0.75734636 0.76100936        nan 0.75734636        nan 0.76100936\n",
      " 0.75734636 0.76100936 0.76100936 0.76100936 0.75734636 0.76100936\n",
      " 0.76100936        nan        nan 0.75734636 0.84542125        nan\n",
      " 0.76100936 0.76100936 0.86760277 0.75734636 0.75734636 0.75734636\n",
      " 0.75734636        nan        nan 0.76100936        nan 0.75734636\n",
      " 0.76100936 0.76100936 0.75734636 0.75734636 0.76100936 0.75734636\n",
      " 0.76100936        nan 0.76100936 0.75734636        nan        nan\n",
      " 0.75734636 0.75734636 0.76100936 0.76100936 0.75734636        nan\n",
      " 0.75734636        nan 0.84542125 0.87126577 0.75734636 0.87126577\n",
      " 0.76100936        nan        nan 0.76100936 0.76100936 0.76100936\n",
      " 0.75734636        nan        nan 0.76100936 0.75734636 0.76100936\n",
      " 0.75734636 0.75734636        nan 0.86027676        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.76100936\n",
      "        nan 0.76100936 0.84542125 0.76100936 0.76100936        nan\n",
      " 0.81595442 0.75734636        nan 0.75734636 0.75734636 0.75734636\n",
      " 0.76100936        nan        nan 0.75734636 0.76100936        nan\n",
      "        nan 0.76100936 0.76100936 0.75734636        nan 0.75734636\n",
      " 0.75734636        nan        nan 0.75734636        nan        nan\n",
      "        nan        nan 0.75734636        nan 0.75734636        nan\n",
      "        nan        nan        nan 0.75734636        nan 0.76100936\n",
      "        nan 0.76100936 0.76100936        nan 0.81595442 0.76100936\n",
      "        nan 0.76100936        nan        nan 0.75734636        nan\n",
      " 0.76100936 0.75734636 0.81595442 0.81595442 0.76100936        nan\n",
      " 0.76100936 0.76100936 0.76100936        nan        nan 0.75734636\n",
      "        nan 0.75734636 0.75734636        nan 0.75734636        nan\n",
      " 0.76100936        nan        nan        nan        nan        nan\n",
      "        nan 0.84542125 0.76100936 0.75734636 0.75734636        nan\n",
      " 0.75734636        nan 0.76100936 0.76100936 0.87126577 0.76100936\n",
      "        nan 0.81595442        nan        nan 0.75734636 0.76100936\n",
      " 0.76100936 0.87118437 0.76100936 0.76100936 0.75734636        nan\n",
      " 0.76100936 0.81595442 0.75734636        nan 0.76100936        nan\n",
      " 0.75734636 0.76100936 0.76100936 0.76100936        nan 0.75734636\n",
      " 0.75734636        nan 0.87126577        nan 0.75734636 0.76100936\n",
      "        nan        nan 0.75734636 0.86027676 0.87118437        nan\n",
      " 0.84542125 0.75734636        nan 0.76100936 0.75734636 0.75734636\n",
      "        nan 0.87126577 0.86760277 0.75734636 0.75734636        nan\n",
      " 0.75734636 0.75734636 0.76100936 0.75734636 0.75734636        nan\n",
      "        nan 0.87126577 0.75734636 0.75734636        nan 0.75734636\n",
      " 0.75734636 0.75734636        nan        nan        nan 0.75734636\n",
      "        nan        nan 0.76100936 0.75734636 0.75734636 0.76100936\n",
      " 0.87118437 0.76100936        nan        nan 0.75734636 0.87118437\n",
      "        nan 0.86027676 0.76100936        nan 0.75734636 0.76100936\n",
      "        nan 0.76100936        nan 0.76100936        nan 0.84542125\n",
      " 0.75734636        nan 0.75734636        nan 0.76100936        nan\n",
      " 0.76100936        nan 0.75734636 0.76100936        nan        nan\n",
      " 0.75734636 0.75734636 0.75734636 0.75734636 0.76100936        nan\n",
      " 0.75734636        nan        nan 0.75734636 0.75734636 0.86027676\n",
      " 0.76100936        nan 0.75734636 0.75734636 0.75734636        nan\n",
      "        nan 0.75734636 0.81595442 0.76100936        nan        nan\n",
      "        nan 0.75734636 0.75734636 0.75734636        nan        nan\n",
      "        nan 0.75734636 0.76100936 0.76100936 0.84542125        nan\n",
      "        nan 0.75734636 0.76100936 0.75734636 0.75734636 0.75734636\n",
      "        nan 0.76100936 0.75734636 0.76100936        nan        nan\n",
      " 0.76100936 0.76100936 0.81595442 0.75734636 0.87118437 0.76100936\n",
      " 0.76100936        nan 0.75734636 0.87118437 0.75734636 0.76100936\n",
      " 0.76100936        nan 0.76100936 0.87118437 0.86760277        nan\n",
      " 0.75734636 0.84542125        nan        nan 0.76100936 0.76100936\n",
      "        nan 0.76100936 0.75734636 0.75734636 0.75734636 0.87126577\n",
      " 0.75734636 0.76100936 0.87118437        nan 0.76100936 0.76100936\n",
      "        nan 0.75734636 0.76100936 0.76100936 0.76100936 0.76100936\n",
      " 0.76100936 0.76100936        nan 0.76100936 0.75734636 0.75734636\n",
      "        nan        nan 0.84542125        nan        nan        nan\n",
      "        nan 0.75734636 0.87118437 0.75734636        nan        nan\n",
      "        nan 0.76100936        nan 0.75734636        nan 0.75734636\n",
      " 0.76100936 0.75734636        nan        nan 0.76100936 0.75734636\n",
      " 0.76100936 0.76100936 0.87126577        nan        nan        nan\n",
      "        nan 0.76100936 0.81595442 0.76100936 0.75734636 0.86760277\n",
      " 0.76100936 0.81595442        nan 0.76100936 0.81595442        nan\n",
      "        nan        nan 0.76100936 0.75734636 0.84542125        nan\n",
      "        nan 0.75734636        nan 0.75734636        nan        nan\n",
      " 0.75734636        nan        nan 0.87126577 0.75734636 0.75734636\n",
      "        nan 0.75734636        nan        nan 0.75734636 0.76100936\n",
      " 0.75734636 0.86760277 0.84542125 0.76100936        nan        nan\n",
      " 0.76100936        nan        nan 0.76100936 0.75734636        nan\n",
      "        nan        nan 0.76100936 0.75734636        nan 0.86027676\n",
      "        nan 0.75734636        nan 0.76100936 0.75734636        nan\n",
      "        nan 0.75734636 0.76100936 0.76100936 0.87126577        nan\n",
      " 0.76100936 0.76100936 0.76100936 0.75734636 0.76100936 0.75734636\n",
      "        nan        nan 0.75734636 0.87118437 0.76100936        nan\n",
      "        nan 0.75734636 0.84542125 0.76100936 0.75734636 0.75734636\n",
      "        nan        nan        nan 0.84542125        nan        nan\n",
      " 0.75734636 0.76100936        nan 0.76100936 0.87118437 0.75734636\n",
      "        nan        nan 0.75734636 0.75734636        nan 0.87126577\n",
      "        nan        nan        nan        nan 0.86760277        nan\n",
      " 0.87126577        nan 0.86027676 0.75734636        nan 0.76100936\n",
      "        nan        nan 0.76100936 0.84542125 0.75734636 0.76100936\n",
      " 0.86760277 0.75734636 0.87118437 0.86027676 0.87118437 0.76100936\n",
      "        nan 0.75734636        nan 0.76100936 0.76100936 0.75734636\n",
      " 0.75734636        nan 0.86027676 0.75734636        nan        nan\n",
      "        nan 0.75734636 0.76100936 0.76100936 0.76100936 0.75734636\n",
      "        nan 0.76100936 0.86027676        nan        nan 0.75734636\n",
      "        nan 0.76100936        nan        nan 0.76100936        nan\n",
      " 0.76100936 0.76100936        nan        nan 0.76100936        nan\n",
      " 0.75734636 0.76100936        nan        nan 0.86027676        nan\n",
      " 0.75734636 0.76100936        nan 0.76100936 0.87118437 0.84542125\n",
      " 0.75734636 0.76100936        nan        nan 0.76100936 0.75734636\n",
      "        nan 0.76100936        nan        nan        nan        nan\n",
      " 0.76100936 0.75734636 0.76100936 0.76100936 0.76100936        nan\n",
      " 0.75734636 0.75734636        nan        nan 0.75734636 0.76100936\n",
      " 0.87126577 0.76100936 0.75734636 0.76100936        nan 0.87126577\n",
      " 0.75734636 0.75734636        nan        nan 0.76100936 0.75734636\n",
      " 0.75734636 0.84542125        nan 0.75734636]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(200,800)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr, param_distributions=param_grid, cv=kfolds, n_iter=700,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The {score_measure} score is {rand_search.best_score_} with {rand_search.best_params_} parameters\")\n",
    "\n",
    "\n",
    "bestlr = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81b2fcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8305085 Precision=0.8541667 Recall=0.9318182 F1=0.8913043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4465c0",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06d9fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 400 candidates, totalling 1200 fits\n",
      "The accuracy score is 0.8528180354267311 with parameters: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 381, 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 3\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from  Random Search to perform the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-100,min_iter+100)\n",
    "}\n",
    "\n",
    "logreg =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = logreg, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The {score_measure} score is {rand_search.best_score_} with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlogreg = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "175bf863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8305085 Precision=0.8541667 Recall=0.9318182 F1=0.8913043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54ba75",
   "metadata": {},
   "source": [
    "# SVM Classification model with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acfa1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_lin_model = SVC(kernel=\"linear\", probability=True)\n",
    "_ = svm_lin_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b10127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the performance DataFrame\n",
    "performance = pd.DataFrame(columns=['model', 'Accuracy', 'Precision', 'Recall', 'F1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77a0c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with linear kernel</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  Accuracy  Precision    Recall        F1\n",
       "0  svm with linear kernel  0.881356   0.877551  0.977273  0.924731"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_preds = svm_lin_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm with linear kernel\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00d5a3",
   "metadata": {},
   "source": [
    "# SVM Classification model with rbf Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7ed44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf_model = SVC(kernel=\"rbf\", C=4, gamma='scale', probability=True)\n",
    "_ = svm_rbf_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86200e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with linear kernel</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with rbf kernel</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.886598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  Accuracy  Precision    Recall        F1\n",
       "0  svm with linear kernel  0.881356   0.877551  0.977273  0.924731\n",
       "0     svm with rbf kernel  0.813559   0.811321  0.977273  0.886598"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_preds = svm_rbf_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm with rbf kernel\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb325f",
   "metadata": {},
   "source": [
    "# SVM Classification model with Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93bcc3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly_model = SVC(kernel=\"poly\", degree=3, coef0=1, C=4, probability=True)\n",
    "_ = svm_poly_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00e7a20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with linear kernel</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with rbf kernel</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.886598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  Accuracy  Precision    Recall        F1\n",
       "0      svm with linear kernel  0.881356   0.877551  0.977273  0.924731\n",
       "0         svm with rbf kernel  0.813559   0.811321  0.977273  0.886598\n",
       "0  svm with polynomial kernel  0.796610   0.807692  0.954545  0.875000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_preds = svm_poly_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "\n",
    "if TP+FP == 0:\n",
    "    precision = 0\n",
    "else:\n",
    "    precision = TP/(TP+FP)\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm with polynomial kernel\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [precision], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65280b81",
   "metadata": {},
   "source": [
    "# SVM Classification model with Randomized Search Kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a35bc6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.857 total time=   0.2s\n",
      "[CV 2/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.852 total time=   0.2s\n",
      "[CV 3/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.815 total time=   0.4s\n",
      "[CV 4/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.852 total time=   0.4s\n",
      "[CV 5/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.778 total time=   0.4s\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.852 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.963 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.815 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.889 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.704 total time=   0.0s\n",
      "[CV 1/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.963 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.815 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.889 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.704 total time=   0.0s\n",
      "[CV 1/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.893 total time=   0.0s\n",
      "[CV 2/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.963 total time=   0.2s\n",
      "[CV 3/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.889 total time=   0.1s\n",
      "[CV 5/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.741 total time=   0.0s\n",
      "[CV 1/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.893 total time=   0.0s\n",
      "[CV 2/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.963 total time=   0.2s\n",
      "[CV 3/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.889 total time=   0.1s\n",
      "[CV 5/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.852 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.893 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.963 total time=   0.2s\n",
      "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.889 total time=   0.1s\n",
      "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.893 total time=   0.0s\n",
      "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.852 total time=   0.0s\n",
      "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.815 total time=   0.0s\n",
      "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.889 total time=   0.0s\n",
      "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "The best accuracy score is 0.8748677248677248\n",
      "... with parameters: {'kernel': 'linear', 'gamma': 0.001, 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 3\n",
    "param_grid = {'C': [0.1, 1, 1], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001],\n",
    "              'kernel': ['linear','poly','rbf']} \n",
    "  \n",
    "rand_search = RandomizedSearchCV(SVC(), param_grid, refit=True, verbose=3)\n",
    "  \n",
    "# fitting the model for randomized search\n",
    "rand_search.fit(X_train, y_train)\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cbccf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with linear kernel</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with rbf kernel</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.886598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random search SVM Linear</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  Accuracy  Precision    Recall        F1\n",
       "0      svm with linear kernel  0.881356   0.877551  0.977273  0.924731\n",
       "0         svm with rbf kernel  0.813559   0.811321  0.977273  0.886598\n",
       "0  svm with polynomial kernel  0.796610   0.807692  0.954545  0.875000\n",
       "0    Random search SVM Linear  0.881356   0.877551  0.977273  0.924731"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Random search SVM Linear\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc11a1",
   "metadata": {},
   "source": [
    "# Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e07891e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg_model = LogisticRegression(penalty='none')\n",
    "_ = log_reg_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7057397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 model  accuracy\n",
      "2        decision tree      0.90\n",
      "0  logistic regression      0.80\n",
      "1                  SVM      0.75\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'model': ['logistic regression', 'SVM', 'decision tree'], 'accuracy': [0.8, 0.75, 0.9]})\n",
    "\n",
    "# sort the DataFrame by the 'accuracy' column in descending order\n",
    "df_sorted = df.sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "# print the sorted DataFrame\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0eaa4369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with linear kernel</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with rbf kernel</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.886598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random search SVM Linear</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default logistic</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  Accuracy  Precision    Recall        F1\n",
       "0      svm with linear kernel  0.881356   0.877551  0.977273  0.924731\n",
       "0         svm with rbf kernel  0.813559   0.811321  0.977273  0.886598\n",
       "0  svm with polynomial kernel  0.796610   0.807692  0.954545  0.875000\n",
       "0    Random search SVM Linear  0.881356   0.877551  0.977273  0.924731\n",
       "0            default logistic  0.830508   0.854167  0.931818  0.891304"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_preds = log_reg_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"default logistic\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febebc97",
   "metadata": {},
   "source": [
    "# RandomizedSearch with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ef6f1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=0.001, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.001, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.001, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.001, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.001, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.001, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=1, penalty=l1, solver=liblinear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .C=1, penalty=l1, solver=liblinear;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END .C=1, penalty=l1, solver=liblinear;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END .C=1, penalty=l1, solver=liblinear;, score=0.926 total time=   0.0s\n",
      "[CV 5/5] END .C=1, penalty=l1, solver=liblinear;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.750 total time=   0.0s\n",
      "[CV 2/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.778 total time=   0.0s\n",
      "[CV 3/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.741 total time=   0.0s\n",
      "[CV 5/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.741 total time=   0.0s\n",
      "The best accuracy score is 0.8603174603174605\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "40 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got elastic.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got lasso.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      " 0.86031746        nan        nan 0.75740741]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "LR=LogisticRegression()\n",
    "kfolds = 5\n",
    "param_grid = {'C': [0.1, 1, 10,0.001], \n",
    "              \"solver\" : [ 'lbfgs', 'liblinear'],\n",
    "              \"penalty\" : ['l1','l2','lasso','elastic']} \n",
    "  \n",
    "grid = RandomizedSearchCV(LR, param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"The best {score_measure} score is {grid.best_score_}\")\n",
    "print(f\"... with parameters: {grid.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c3b4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = grid.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression Randomised\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281924c9",
   "metadata": {},
   "source": [
    "# Logistic Regression with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "836f7f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV 1/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.852 total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.852 total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.741 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.821 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.741 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.889 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.704 total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.852 total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.815 total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.852 total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.741 total time=   0.0s\n",
      "[CV 1/5] END ..C=0.1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ..C=0.1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ..C=0.1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ..C=0.1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ..C=0.1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=1, penalty=l1, solver=liblinear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .C=1, penalty=l1, solver=liblinear;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END .C=1, penalty=l1, solver=liblinear;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END .C=1, penalty=l1, solver=liblinear;, score=0.926 total time=   0.0s\n",
      "[CV 5/5] END .C=1, penalty=l1, solver=liblinear;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.893 total time=   0.0s\n",
      "[CV 2/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.926 total time=   0.0s\n",
      "[CV 3/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.815 total time=   0.0s\n",
      "[CV 4/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.889 total time=   0.0s\n",
      "[CV 5/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END .C=1, penalty=l2, solver=liblinear;, score=0.893 total time=   0.0s\n",
      "[CV 2/5] END .C=1, penalty=l2, solver=liblinear;, score=0.963 total time=   0.0s\n",
      "[CV 3/5] END .C=1, penalty=l2, solver=liblinear;, score=0.815 total time=   0.0s\n",
      "[CV 4/5] END .C=1, penalty=l2, solver=liblinear;, score=0.889 total time=   0.0s\n",
      "[CV 5/5] END .C=1, penalty=l2, solver=liblinear;, score=0.741 total time=   0.0s\n",
      "[CV 1/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=1, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ..C=1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ..C=1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ..C=1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ..C=1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ..C=1, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ......C=10, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10, penalty=l1, solver=liblinear;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=10, penalty=l1, solver=liblinear;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END C=10, penalty=l1, solver=liblinear;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END C=10, penalty=l1, solver=liblinear;, score=0.926 total time=   0.0s\n",
      "[CV 5/5] END C=10, penalty=l1, solver=liblinear;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.926 total time=   0.0s\n",
      "[CV 5/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END C=10, penalty=l2, solver=liblinear;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=10, penalty=l2, solver=liblinear;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END C=10, penalty=l2, solver=liblinear;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END C=10, penalty=l2, solver=liblinear;, score=0.926 total time=   0.0s\n",
      "[CV 5/5] END C=10, penalty=l2, solver=liblinear;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=10, penalty=lasso, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=10, penalty=lasso, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=10, penalty=elastic, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=10, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=10, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=10, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=10, penalty=elastic, solver=liblinear;, score=nan total time=   0.0s\n",
      "The best accuracy score is 0.8603174603174605\n",
      "... with parameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "75 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got lasso.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got elastic.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.83068783 0.80873016 0.82328042        nan        nan\n",
      "        nan        nan        nan 0.86031746 0.86005291 0.86005291\n",
      "        nan        nan        nan        nan        nan 0.84603175\n",
      " 0.84603175 0.84603175        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "param_grid = {'C': [0.1, 1, 10], \n",
    "              'solver' : [ 'lbfgs', 'liblinear'],\n",
    "              'penalty' : ['l1','l2','lasso','elastic']} \n",
    "  \n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"The best {score_measure} score is {grid.best_score_}\")\n",
    "print(f\"... with parameters: {grid.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "048660b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = grid.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression Grid\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8e8e5",
   "metadata": {},
   "source": [
    "# Decision tree model using the randomsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a8984fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "The best accuracy score is 0.8383252818035426\n",
      "... with parameters: {'min_samples_split': 10, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0056, 'max_leaf_nodes': 119, 'max_depth': 15, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "12 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.83800322        nan 0.83800322 0.71980676 0.72028986 0.75732689\n",
      " 0.71980676 0.7568438  0.73462158 0.75732689 0.74154589 0.72028986\n",
      " 0.68309179 0.72705314 0.80901771 0.77938808 0.83059581 0.75732689\n",
      " 0.80901771 0.69758454 0.72028986 0.75732689 0.7568438  0.72028986\n",
      " 0.69806763 0.72769726 0.72753623 0.75732689 0.69033816 0.72753623\n",
      " 0.80901771 0.75732689 0.69033816 0.73429952 0.75732689 0.71980676\n",
      " 0.83832528 0.74202899 0.77874396 0.71980676 0.73462158 0.83800322\n",
      " 0.72028986 0.83800322 0.75732689 0.72028986        nan 0.69033816\n",
      " 0.75732689 0.72753623 0.71239936 0.8310789  0.75732689 0.72028986\n",
      " 0.80837359 0.72769726 0.69806763 0.71980676 0.80177134 0.75732689\n",
      " 0.79355878 0.72028986 0.75732689 0.77198068 0.75732689 0.71980676\n",
      " 0.75732689 0.80161031 0.71980676 0.75732689 0.72028986        nan\n",
      " 0.71980676 0.75732689        nan 0.74959742 0.83091787 0.72028986\n",
      " 0.71980676 0.72028986 0.83800322 0.71980676 0.79371981 0.80901771\n",
      " 0.69758454 0.69033816 0.7568438  0.72028986 0.73429952 0.82318841\n",
      " 0.75732689 0.69033816 0.75732689 0.71256039 0.7568438  0.75732689\n",
      " 0.71980676 0.72028986 0.72028986 0.72705314]\n",
      "  warnings.warn(\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.93007733        nan 0.87501018 0.83817664 0.78298738 0.75734636\n",
      " 0.83451363 0.84183964 0.82714693 0.75734636 0.84928775 0.78298738\n",
      " 0.77936508 0.84558405 0.86760277 0.9042328  0.92275132 0.75734636\n",
      " 0.86760277 0.81986162 0.78298738 0.75734636 0.84183964 0.78298738\n",
      " 0.81595442 0.75734636 0.76467236 0.75734636 0.80508751 0.77203907\n",
      " 0.86760277 0.75734636 0.79409849 0.84928775 0.75734636 0.83817664\n",
      " 0.97798128 0.7978022  0.89344729 0.83817664 0.82714693 0.95217745\n",
      " 0.76833537 0.87501018 0.75734636 0.76833537        nan 0.80875051\n",
      " 0.75734636 0.76467236 0.83085063 0.98530729 0.75734636 0.76833537\n",
      " 0.92275132 0.75734636 0.81595442 0.83817664 0.83426943 0.75734636\n",
      " 0.89344729 0.78298738 0.75734636 0.93748474 0.75734636 0.83451363\n",
      " 0.75734636 0.86760277 0.83817664 0.75734636 0.78298738        nan\n",
      " 0.82714693 0.75734636        nan 0.85295075 0.97798128 0.78298738\n",
      " 0.83817664 0.76467236 0.87501018 0.83817664 0.88237688 0.86760277\n",
      " 0.81986162 0.80508751 0.84183964 0.76833537 0.84928775 0.93007733\n",
      " 0.75734636 0.80875051 0.75734636 0.85669516 0.84183964 0.75734636\n",
      " 0.83817664 0.76100936 0.76833537 0.84558405]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,60),  \n",
    "    'min_samples_leaf': np.arange(1,50),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "503e59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision tree random search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d39b5",
   "metadata": {},
   "source": [
    "# Decision tree model using the Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0adbae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2268 candidates, totalling 6804 fits\n",
      "The best accuracy score is 0.8380032206119163\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 194, 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 5, 'min_samples_split': 25}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(25,32),  \n",
    "    'min_samples_leaf': np.arange(3,6),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.0004, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(194,200), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "398f6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "\n",
    "if TP+FP == 0:\n",
    "    precision = 0  # or precision = np.nan\n",
    "else:\n",
    "    precision = TP/(TP+FP)\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Grid search DT\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [precision], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0dd9a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 model  accuracy\n",
      "2        decision tree      0.90\n",
      "0  logistic regression      0.80\n",
      "1                  SVM      0.75\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'model': ['logistic regression', 'SVM', 'decision tree'], 'accuracy': [0.8, 0.75, 0.9]})\n",
    "\n",
    "# sort the DataFrame by the 'accuracy' column in descending order\n",
    "df_sorted = df.sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "# print the sorted DataFrame\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af65505a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with rbf kernel</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.886598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default logistic</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression Randomised</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression Grid</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grid search DT</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.883721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision tree random search</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.898876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with linear kernel</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random search SVM Linear</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  Accuracy  Precision    Recall        F1\n",
       "0      svm with polynomial kernel  0.796610   0.807692  0.954545  0.875000\n",
       "0             svm with rbf kernel  0.813559   0.811321  0.977273  0.886598\n",
       "0                default logistic  0.830508   0.854167  0.931818  0.891304\n",
       "0  Logistic Regression Randomised  0.830508   0.854167  0.931818  0.891304\n",
       "0        Logistic Regression Grid  0.830508   0.854167  0.931818  0.891304\n",
       "0                  Grid search DT  0.830508   0.904762  0.863636  0.883721\n",
       "0     Decision tree random search  0.847458   0.888889  0.909091  0.898876\n",
       "0          svm with linear kernel  0.881356   0.877551  0.977273  0.924731\n",
       "0        Random search SVM Linear  0.881356   0.877551  0.977273  0.924731"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.sort_values(by =['Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf564d8",
   "metadata": {},
   "source": [
    "Upon comparing the performance of various machine learning models on the Parkinson dataset, I found that SVM with a linear kernel achieved the highest accuracy. This result suggests that SVM is a powerful algorithm for accurately classifying Parkinson's disease in patients. Further research may be warranted to investigate the underlying factors contributing to the superior performance of SVM in this particular application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c389b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "234f587a",
   "metadata": {},
   "source": [
    "# Neural Network  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "240d43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9c40a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 571 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ann = MLPClassifier(hidden_layer_sizes=(60,50,40), solver='adam', max_iter=200)\n",
    "_ = ann.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f718b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfed0ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.40      0.50        15\n",
      "           1       0.82      0.93      0.87        44\n",
      "\n",
      "    accuracy                           0.80        59\n",
      "   macro avg       0.74      0.67      0.69        59\n",
      "weighted avg       0.78      0.80      0.78        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ce376",
   "metadata": {},
   "source": [
    "# With RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ab64089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "{'solver': 'adam', 'max_iter': 5000, 'learning_rate_init': 0.001, 'learning_rate': 'constant', 'hidden_layer_sizes': (50,), 'alpha': 1, 'activation': 'relu'}\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,),(50,30), (40,20), (60,40, 20), (70,50,40)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = RandomizedSearchCV(estimator = ann, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84cf6a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        15\n",
      "           1       0.75      1.00      0.85        44\n",
      "\n",
      "    accuracy                           0.75        59\n",
      "   macro avg       0.37      0.50      0.43        59\n",
      "weighted avg       0.56      0.75      0.64        59\n",
      "\n",
      "Wall time: 15.6 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\santo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4ff4c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with linear kernel</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with rbf kernel</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.886598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random search SVM Linear</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default logistic</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression Randomised</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression Grid</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision tree random search</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.898876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grid search DT</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.883721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Randomized search DT</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grid search DT</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Network Randomized search DT</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model  Accuracy  Precision    Recall  \\\n",
       "0               svm with linear kernel  0.881356   0.877551  0.977273   \n",
       "0                  svm with rbf kernel  0.813559   0.811321  0.977273   \n",
       "0           svm with polynomial kernel  0.796610   0.807692  0.954545   \n",
       "0             Random search SVM Linear  0.881356   0.877551  0.977273   \n",
       "0                     default logistic  0.830508   0.854167  0.931818   \n",
       "0       Logistic Regression Randomised  0.830508   0.854167  0.931818   \n",
       "0             Logistic Regression Grid  0.830508   0.854167  0.931818   \n",
       "0          Decision tree random search  0.847458   0.888889  0.909091   \n",
       "0                       Grid search DT  0.830508   0.904762  0.863636   \n",
       "0                 Randomized search DT  0.864407   0.875000  0.954545   \n",
       "0                       Grid search DT  0.847458   0.857143  0.954545   \n",
       "0  Neural Network Randomized search DT  0.745763   0.745763  1.000000   \n",
       "\n",
       "         F1  \n",
       "0  0.924731  \n",
       "0  0.886598  \n",
       "0  0.875000  \n",
       "0  0.924731  \n",
       "0  0.891304  \n",
       "0  0.891304  \n",
       "0  0.891304  \n",
       "0  0.898876  \n",
       "0  0.883721  \n",
       "0  0.913043  \n",
       "0  0.903226  \n",
       "0  0.854369  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "\n",
    "if TP+FP == 0:\n",
    "    precision = 0  # or precision = np.nan\n",
    "else:\n",
    "    precision = TP/(TP+FP)\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Neural Network Randomized search DT\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [precision], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba4679",
   "metadata": {},
   "source": [
    "# With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c22511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "{'activation': 'relu', 'alpha': 0.7, 'hidden_layer_sizes': (90,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.005, 'max_iter': 5000, 'solver': 'adam'}\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (30,), (50,), (70,), (90,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [.5, .7, 1],\n",
    "    'learning_rate': ['adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.005, 0.01, 0.15],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = GridSearchCV(estimator = ann, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f13e83ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.40      0.55        15\n",
      "           1       0.83      0.98      0.90        44\n",
      "\n",
      "    accuracy                           0.83        59\n",
      "   macro avg       0.84      0.69      0.72        59\n",
      "weighted avg       0.83      0.83      0.81        59\n",
      "\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1612b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "\n",
    "if TP+FP == 0:\n",
    "    precision = 0  # or precision = np.nan\n",
    "else:\n",
    "    precision = TP/(TP+FP)\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Neural Network Grid search DT\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [precision], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8ad58b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with linear kernel</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with rbf kernel</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.886598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random search SVM Linear</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default logistic</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression Randomised</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression Grid</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision tree random search</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.898876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grid search DT</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.883721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Randomized search DT</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grid search DT</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Network Randomized search DT</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Network Grid search DT</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model  Accuracy  Precision    Recall  \\\n",
       "0               svm with linear kernel  0.881356   0.877551  0.977273   \n",
       "0                  svm with rbf kernel  0.813559   0.811321  0.977273   \n",
       "0           svm with polynomial kernel  0.796610   0.807692  0.954545   \n",
       "0             Random search SVM Linear  0.881356   0.877551  0.977273   \n",
       "0                     default logistic  0.830508   0.854167  0.931818   \n",
       "0       Logistic Regression Randomised  0.830508   0.854167  0.931818   \n",
       "0             Logistic Regression Grid  0.830508   0.854167  0.931818   \n",
       "0          Decision tree random search  0.847458   0.888889  0.909091   \n",
       "0                       Grid search DT  0.830508   0.904762  0.863636   \n",
       "0                 Randomized search DT  0.864407   0.875000  0.954545   \n",
       "0                       Grid search DT  0.847458   0.857143  0.954545   \n",
       "0  Neural Network Randomized search DT  0.745763   0.745763  1.000000   \n",
       "0        Neural Network Grid search DT  0.830508   0.826923  0.977273   \n",
       "\n",
       "         F1  \n",
       "0  0.924731  \n",
       "0  0.886598  \n",
       "0  0.875000  \n",
       "0  0.924731  \n",
       "0  0.891304  \n",
       "0  0.891304  \n",
       "0  0.891304  \n",
       "0  0.898876  \n",
       "0  0.883721  \n",
       "0  0.913043  \n",
       "0  0.903226  \n",
       "0  0.854369  \n",
       "0  0.895833  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc1dd1",
   "metadata": {},
   "source": [
    "Based on the given accuracy scores, the Neural Network (NN) model appears to have performed relatively poorly compared to the other models. The NN model's accuracy using Randomized search was 0.745763, which is lower than all the other models' accuracy. On the other hand, the NN model's accuracy using Grid search was 0.830508, which is the same as that of Logistic Regression.\n",
    "\n",
    "Compared to the SVM models, the NN model's accuracy was significantly lower. The linear kernel-based SVM model had an accuracy of 0.881356, which was the highest among all models. The polynomial kernel-based SVM model also had the same accuracy as the linear kernel-based SVM model. The RBF kernel-based SVM model had an accuracy of 0.813559. The Decision Tree model's accuracy was relatively high, with the Randomized search approach having an accuracy of 0.847458 and the Grid search approach having an accuracy of 0.830508.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f78c26",
   "metadata": {},
   "source": [
    "It is important to note that the choice of model and the corresponding hyperparameters depend on the nature of the data and the problem at hand. While the NN model may have performed relatively poorly in this particular scenario, it may outperform the other models in different contexts. Hence, it is crucial to consider the strengths and weaknesses of different models and select the one that suits the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863a30b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
